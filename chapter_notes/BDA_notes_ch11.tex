\documentclass[a4paper,11pt,english]{article}

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{url}
\urlstyle{same}

\usepackage[bookmarks=false]{hyperref}
\hypersetup{%
  bookmarksopen=true,
  bookmarksnumbered=true,
  pdftitle={Bayesian data analysis},
  pdfsubject={Comments},
  pdfauthor={Aki Vehtari},
  pdfkeywords={Bayesian probability theory, Bayesian inference, Bayesian data analysis},
  pdfstartview={FitH -32768},
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  filecolor=black,
  urlcolor=black
}


% if not draft, smaller printable area makes the paper more readable
\topmargin -4mm
\oddsidemargin 0mm
\textheight 225mm
\textwidth 160mm

%\parskip=\baselineskip

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
\DeclareMathOperator{\NInvchi2}{N-Inv-\chi^2}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\tr}{tr}
%\DeclareMathOperator{\Pr}{Pr}
\DeclareMathOperator{\trace}{trace}

\def\eff{\mathrm{eff}}

\pagestyle{empty}

\begin{document}
\thispagestyle{empty}

\section*{Bayesian data analysis -- reading instructions 11} 
\smallskip
{\bf Aki Vehtari}
\smallskip

\subsection*{Chapter 11}

Outline of the chapter 11
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item 11.1 Gibbs sampler
\item 11.2 Metropolis and Metropolis-Hastings
\item 11.3 Using Gibbs and Metropolis as building blocks
\item 11.4 Inference and assessing convergence
\item 11.5 Effective number of simulation draws
\item 11.6 Example: hierarchical normal model (skip this)
\end{list}

Demos (unfortunately not all demos have been ported to Python and R yet)
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item demo11\_1: Gibbs sampling
\item demo11\_2: Metropolis sampling
\item demo11\_3: Convergence of Markov chain
\item demo11\_4: potential scale reduction $\hat{R}$
\item demo11\_5: effective number of samples $n_{\mathrm{eff}}$ and thinning
\end{list}

Find all the terms and symbols listed below. When reading the chapter,
write down questions related to things unclear for you or things you
think might be unclear for others. 
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item Markov chain
\item Markov chain Monte Carlo
\item random walk
\item starting point
\item transition distribution
\item jumping / proposal distribution
\item to converge, convergence, assessing convergence
\item stationary distribution, stationarity
\item effective number of simulations
\item Gibbs sampler
\item Metropolis sampling / algorithm
\item Metropolis-Hastings algorithm
\item acceptance / rejection rule
\item acceptance / rejection rate
%\item (irreducible, aperiodic and not-tranisient are explained in slides)
\item within-sequence correlation, serial correlation
\item warm-up / burn-in
\item to thin, thinned
\item overdispersed starting points
\item mixing
\item to diagnose convergence
\item between- and within-sequence variances
\item potential scale reduction, $\hat{R}$
\item the variance of the average of a correlated sequence
\item autocorrelation
\item variogram
\item $n_{\mathrm{eff}}$
\end{list}
%fourth edition batching p. 287

\subsection*{Basics of Markov chains}

Slides by J. Virtamo for the course S-38.143 Queueing Theory has nice
review of the fundamental terms and Finnish translations for them (in
English
\url{http://www.netlab.tkk.fi/opetus/s38143/luennot/english.shtml} and
in Finnish
\url{http://www.netlab.hut.fi/opetus/s38143/luennot/index.shtml}). See
specially the slides for the lecture 4. To prove that Metropolis
algorithm works, it is sufficient to show that chain is irreducible,
aperiodic and not transient.

% \subsection*{Otanta vs. poiminta}

% Gibbs-otanta ja Gibbs-poiminta termej‰ k‰ytet‰‰n molempia. Itse
% pyrin k‰ytt‰m‰‰n termi‰ poiminta, koska mielest‰ni on mukavampi
% puhua n‰ytteiden poimimisesta kuin n‰ytteiden ottamisesta tai
% otostamisesta.

\subsection*{Metropolis algorithm}

There is a lot of freedom in selection of proposal distribution in
Metropolis algorithm. There are some restrictions, but we don't go to
the mathematical details in this course.

% Metropolis-alogritmin ehdotusjakauman valinnassa on paljon
% vapautta, mutta sit‰ ei voi valita aivan miten vain. T‰ll‰
% kurssilla riitt‰‰ muistaa, ett‰ kurssilla l‰pik‰ydyt perusjakaumat
% ovat sopivia. Tarkemmat ehdot menev‰t varsin matemaattiseksi.


~\\
Don't confuse rejection in the rejection sampling and in Metropolis
algorithm. In the rejection sampling, the rejected samples are thrown
away. In Metropolis algorithm the rejected proposals are thrown away,
but time moves on and the previous sample x(t) is also the sample
x(t+1).
% Olkaa tarkkoina, ‰lk‰‰ sotkeko hylk‰yspoimintaa ja
% Metropolis-algoritmin hylk‰yst‰. Hylk‰yspoiminnassa hyl‰tyt heitet‰‰n
% pois ja Metropoliksessakin hyl‰tyt {\em ehdotukset} heitet‰‰n pois,
% mutta silti aika liikkuu eteenp‰in ja edellinen piste x(t) on myˆs
% uuden ajanhetken piste x(t+1).

~\\
When rejecting a proposal, the previous sample is repeated in the
chain, they have to be included and they are valid samples from the
distribution. For basic Metropolis, it can be shown that optimal
rejection rate is 55--77\%, so that on even the optimal case quite many
of the samples are repeated samples. However, high number of
rejections is acceptable as then the accepted proposals are on average
further away from the previous point. It is better to jump further
away 23--45\% of time than more often to jump really close. Methods for
estimating the effective sample size are useful for measuring how
effective a given chain is.

% Paikallaan pysytt‰ess‰ saadut n‰ytteet on pidett‰v‰ mukana, ja ne
% ovat n‰ytteit‰ jakaumasta. Optimaalinen hylk‰ysprosentti on
% 55-77\%, eli optimaalisessakin tapauksessa 55-77\% kerroista
% pysyt‰‰n paikoillaan. T‰rkeint‰h‰n t‰ss‰ ei ole tuo hylk‰ysten
% m‰‰r‰ sin‰ns‰ vaan, kun ehdotusjakauma on leve‰mpi pystyt‰‰n
% yhdell‰ hypyll‰ etenem‰‰n pitk‰lle. Eli on parempi 23-45\%
% tapauksissa hyp‰t‰ pitk‰lle kuin useammin hyp‰t‰ l‰helle. Jos
% ehdotusjakauma on tosi leve‰, hylk‰yksi‰ tulee tietenkin hyvin
% paljon, ja silloin ketjun autokorrelaatio kasvaa ja efektiivisten
% n‰ytteiden m‰‰r‰ j‰‰ pieneksi. Voitte itse kokeilla ehdotusjakamuan
% leveyden vaikutusta autokorrelaatioon. Ensi viikon luennolla
% kerrotuilla menetelmill‰ voi arvioida efektiivisten n‰ytteiden
% m‰‰r‰‰.

% ~\\
% Metropolis-algoritmissa suhdeluku lasketaan edellisen ajanhetken
% pisteen ja ehdotetun pisteen tiheyksien suhteessa, miss‰ tiheys on
% nimenomaan kiinnostavan jakauman tiheys. Symmetrisen
% ehdotusjakauman vuoksi edellinen piste on myˆs ehdotusjakauman
% keskipiste, mutta on parempi silti ajatella sit‰ vain edellisen‰
% n‰ytteen‰ kiinnostavasta jakaumasta eik‰ pisteen‰
% ehdotusjakaumasta.

% \subsection*{Metropolis-Hastings-algoritmista}

% Metropolis-Hastings-algoritmissa ehdotusjakauma ei v‰ltt‰m‰tt‰ ole
% symmetrinen, jolloin suhdeluvussa ovat mukana edellisen ja
% ehdotetun pisteen tiheydet kiinnostavassa jakaumassa ja
% ehdotusjakaumassa.

\subsection*{Transition distribution vs. proposal distribution}

Transition distribution is a property of Markov chain. In Metropolis
algorithm the transition distribution is a mixture of a proposal
distribution and a point mass in the current point. The book uses also
term jumping distribution to refer to proposal distribution.

\subsection*{Convergence}

Theoretical convergence in an infinite time is different than
practical convergence in a finite time. There is no exact moment when
chain has converged and thus it is not possible to detect when the
chain has converged (except for rare \emph{perfect sampling} methods
not discussed in BDA3). The convergence diagnostics can help to find
out if the chain is unlikely to be representative of the target
distribution. Furthermore, even if would be able to start from a
independent sample from the posterior so that chain starts from the
convergence, the mixing can be so slow that we may require very large
number of samples before the samples are representative of the target
distribution.

% Teoreettinen konvergenssi ei kerro koko totuutta k‰yt‰nnˆn
% konvergenssista ‰‰rellisess‰ ajassa.

% ~\\
% BDA3 proposes to throw half of the chain away. This is cautios and
% fine if the computation time is not a problem. If MCMC sampling takes
% a lot of time, it may be useful to throw away only as much as
% necessary based on convergence diagnostics, which may be less than
% half. See demo11\_4.m

~\\
If starting point is selected at or near the mode, less time is needed
to reach the area of essential mass, but still the samples in the
beginning of the chain are not presentative of the true distribution
unless the starting point was somehow samples directly from the target
distribution. 

% Jos alkupiste valitaan oleellisen massan alueelta tai jopa moodin
% huipusta, aikaa ei mene siihen, ett‰ ketju saavuttaa oleellisen
% massan alueen, mutta silti ketjun alkup‰‰n n‰ytteet eiv‰t ole
% oikeasta jakaumasta, ellei alkupiste itse ollut poimittu oikeasta
% jakaumasta. Yksitt‰isen pisteen tapauksessa on hankala hahmottaa,
% mit‰ tarkoittaa se, ett‰ se on jostakin jakaumasta, koska
% yksin‰inen on vain yksi piste eik‰ muuta. Mielekk‰‰mm‰ksi asia
% muuttuu kun ajatellaan useita pisteit‰, jotka ovat havaintoja
% jakaumasta ja joita voidaan k‰ytt‰‰ approksimoitaessa integraalia
% jakauman yli. Jos alkupiste valitaan moodin alueelta jotenkin
% muuten kuin poimimalla suoraan jakaumasta ja t‰llaisia ketjuja
% alustetaan $L$ ja ketjuista otetaankin vain alkupisteet, n‰hd‰‰n
% helposti, ett‰ saadut n‰yteet eiv‰t ole oikeasta jakaumasta. Siksi
% sis‰‰najo on tarpeen vaikka alkupisteet olisivat jo oleellisen
% massan alueella. Jos alkupisteet arvotaan suoraan oikeasta
% jakaumasta ovat ketjut v‰littˆm‰sti konvergoituneita (mutta jos
% osataan poimia alkupisteet suoraan oikeasta jakaumasta, ei tarvita
% Markov-ketjuja).

% ~\\
% Kun station‰‰rinen tila on saavutettu ketjun n‰yteet ovat oikeasta
% jakaumasta. Voidaan sanoa, ett‰ ketju konvergoituu station‰‰riseen
% tilaan ja edelleen voidaan tarkastella milloin konvergenssi on
% tapahunut.

\subsection*{$\hat{R}$ ja $n_\eff$}

Note that the method to compute potential scale reductionin $\hat{R}$
and effective number of samples $n_\eff$ has been updated in BDA3. New
compared to BDA2 is splitting of chains and use of autocorrelation in
estimation of $n_\eff$.

Due to randomness in chains, $\hat{R}$ may get values slightly below 1.

%\subsection*{Autocorrelation time}
%REWRITE
%TODO

% acorrtime- ja geyer_imse- funktioiden palauttama luku estimoi
% autokorrelaatioaikaa, joka kertoo alarajan ohennusv‰lille $k$.
% geyer_imse-funktion estimaatti on tarkempi kuin acorrtime:n, koska
% se ottaa huomioon ketjun Markov-ominaisuudet, joista seuraa er‰it‰
% rajoituksia autokorrelaatiofunktion muodolle. 
% Katso lis‰‰ Geyerin paperista, joka lˆytyy kahdeksannen luennon
% materiaalista.

% %~\\
% Autokorrelaatioaika ei liity konvergenssiin muuten kuin, ett‰ hitaasti
% konvergoituvilla ketjuilla on yleens‰ myˆs pitk‰ autokorrelaatioaika,
% koska hidas konvergenssi johtuu usein hitaasta satunnaisk‰velyst‰
% jolloin myˆs n‰ytteet ovat riippuvaisempia.

% \subsection*{Ohennuksesta}

% Autokorrelaatioaika on alaraja ohennuksen $k$:lle.
% Aikasarja-analyyissa Monte Carlo -virhe lasketaan kaikista
% n‰ytteist‰ huomioiden myˆs riippuvista n‰ytteist‰ saatava pieni
% lis‰informaatio. Ohennuksessa osa n‰ytteist‰ heitet‰‰n pois ja
% sitten efektiivinen n‰ytteiden m‰‰r‰ on ohennuksen j‰lkeen
% v‰ltt‰m‰tt‰ pienempi kuin ennen ohennusta. Jos halutaan
% k‰yt‰nn‰llisesti riippumattomia n‰ytteit‰ on ohennetteva niin
% paljon, ett‰ j‰ljelle j‰‰ v‰hemm‰n n‰ytteit‰ kuin
% aikasarja-analyysin arvioima efektivinen n‰ytteiden m‰‰r‰ koko
% ketjusta.

% ~\\
% Moniulotteisessa tapauksessa on turvallisinta laskea
% autokorrelaatioaika kaikista eri suureista ja ohentaa pisimm‰n
% autokorrelaatioajan mukaan.

% \subsection*{Kolmogorov-Smirnov-testi}

% Kolmogorov-Smirnov-testist‰ voi lukea demo. sivulta
% (http://www.physics.csbsju.edu/stats/KS-test.html)
% (Percentile Plot -v‰liotiskkoon asti). Testist‰ riitt‰‰ muistaa
% yleisperiaate, mihin sit‰ k‰ytet‰‰n ja ett‰ se vaatii
% riippumattomat n‰ytteet.

% \subsection*{Useiden ketjujen ajamisesta}

% Useiden ketjujen ajamisen p‰‰syy on konvergenssidiagnostiikan
% helpottaminen. Lis‰ksi riippumattomia ketjuja voidaan k‰ytt‰‰
% ep‰varmuuden arviointiin. Harvemmin riippumattomia ketjuja
% k‰ytet‰‰n riippumattomampien n‰ytteiden poimimiseen, vaikka
% siihenkin sit‰ k‰ytet‰‰n. Jos konvergenssiaika on kohtuullisen
% lyhyt verrattuna autokorrelaatioaikaan, voidaan ajaa $L$
% riippumatonta ketjua ja saada $L$ riippumatonta n‰ytett‰
% (edellytt‰en, ett‰ ketjut ovat konvergoituneet).

\end{document}

%%% Local Variables: 
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
